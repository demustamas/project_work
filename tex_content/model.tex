\section{Description of the model} \label{model}
\subsection{The Autoencoder}
Current problem requires images to be processed, therefore the latent space representation
shall be created by a neural network fitted to work with images as this is expected to learn
the features that might differentiate between non-defective rail images and outliers.
The same applies for the decoder part as well.
Four type of Encoders used as listed below with the consideration to keep the model simple,
allow comparably fast training (which is driven more by the size of the dataset).

The VGG19 is one of the first deep convolutional networks introduced in \cite{simonyan_very_2015}.
In the second model batch normalization was added after each convolutional layer.
The ResNet50 \cite{he_deep_2015} overcomes the issue of the vanishing gradient
by adding the skip connections.
EfficientNetV2 \cite{tan_efficientnetv2_2021} is developed to reduce training time
and provide better parameter efficiency than previous models.

During the encoding the input image sizes are gradually reduced in four steps,
resulting different shapes of the latent space due to the different number of filters applied.
An overview of the resulting latent space considering an input size of 704 x 288
is shown in Table \ref{table:latent_space_shape}.
For comparison the original image of size 720 x 288 x 3 has a total number of 622080 to describe
a single instance.

\begin{table}[!ht]
    \centering
    \begin{tabular}{l c c}
        Encoder type                   & Width x Height x Filters & Number of parameters \\
        \hline
        VGG19                          & 22 x 9 x 512             & 101376               \\
        VGG19 with batch normalization & 22 x 9 x 512             & 101376               \\
        Resnet50                       & 22 x 9 x 2048            & 405504               \\
        EfficientNetV2L                & 22 x 9 x 1280            & 253440               \\
    \end{tabular}
    \caption{Latent space shape of different Encoders}
    \label{table:latent_space_shape}
\end{table}

Once the feature vectors obtained, the original image can be reconstructed.
In current study VGG19 is used as basis decoder model.
As the processing of the data has to be done backwards, the whole structure is
reversed and the convolutional layers are replaces with transposed convolution.

In order to fit the Decoder input to the Encoder output a FilterMatching part is introduced.
As the difference between the latent space and decoder input is only the number of filters,
the shape is adjusted via additional convolutional layers as listed below.
This was done to avoid that during stepping forward on the first layer of the decoder a lot of
filters should be compressed into a much smaller set.
Ideally the decoder and encoder part shall form an hourglass shape, but in this case only a single
model was used for decoding to maintain simplicity.
\begin{itemize}
    \item In case of the VGG models, an identity mapping is applied
    \item For the ResNet50, a two-step approach is followed, each is halving the number of filters
    \item When applying the EfficientNetV2L model, also a two-step decomposition is introduced
          first the number of filters is reduced to 896 and then to 512.
\end{itemize}

\subsection{Anomaly detection}

In the loss-based approach the loss function is set as the pixel-wise mean squared error of the input
and decoded image.
The threshold to designate an image as outlier is to have higher loss value
(distance between input and output image) than the mean loss overall the whole dataset and
three times the standard deviation added together.
This is an initial guess, depending on what outliers are found, the multiplier of the standard
deviation might need to be changed.
Such optimization of the multiplier (or threshold) is only possible when annotation is given.
As this is not the usual case in this problem, an optimization is only possible on manually
labeled dataset and with the assumption that further dataset contains images from the same distribution.
Practically this means that the most common \emph{normal} rails should be included in such
optimization process, including normal track, bridges, junctions, road crossings, etc.
When it is known that no defect are present in the training set, then a limited fine-tuning
is possible, lacking the information on how losses linked to certain type of \emph{normal}
rails differ from each other.

The sklearn implementation of Isolation Forest algorithm described in \cite{liu_isolation_2008} is used
when looking for outliers in the latent space.
The default setting were applied, parameter optimization could be a possibility to
improve the model further, however for comparison purposes this first approach is accepted.

\subsection{Data preprocessing}
During modeling besides augmentation no image manipulations were applied
except the normalization during entering the neural network
and resizing to 704 x 288 x 3 to ensure decoding of the images to the same original size.

\subsection{Model training}
Transfer learning is applied in case of the Encoders to ensure an efficient approach,
the model weights from the Imagenet pretraining is selected for this purpose.
In case of the Decoders no transfer learning is available due to the reversed structure
of the network, there only a random initialization was used.

The loss function is defined as pixel-wise mean squared loss between the original and replicated
images.
Adam optimizer is applied with a learning rate of $5 \cdot 10^{-6}$.
The batch size for training is set to 8.

The dataset is split to \emph{train}, \emph{validation} and \emph{test} set, considering the
imbalance nature of the dataset, during split a stratified approach was applied.
The training of the models were done on the \emph{train} dataset for $100$ epochs,
the \emph{validation} set was used to monitor how well the model generalize and to detect possible
overfitting or other training issues.
The \emph{test} dataset was used for independent analysis of the model, how the results presented
in Section \ref{results} is interpreted over the whole dataset offering the opportunity to compare
which images detected as outliers (or missed) by the different detectors models (loss based or
Isolation Forest).

\subsection{Performance evaluation}
The first approach on the performance evaluation is the general classification metrics based on
the confusion matrices of each approach.
Furthermore, a visualization is provided via a two-step dimensionality reduction approach:
first a PCA was used to determine the 50 most important features of the dataset,
then a t-SNE was applied to find the two most important feature.
This allows a graph representation of the dataset.
This approach was applied on the input images, latent space vectors, filter matched vectors and on
the decoded images, allowing visualization how outliers behave over the whole model.

