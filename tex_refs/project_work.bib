'
@Misc{tomcom_mathemathical_2022,
  author  = {Tamás Demus},
  month   = dec,
  note    = {original-date: 2022-12-02T20:47:30Z},
  title   = {Mathemathical {Modeling} {Practice} - {Railway} track defect detection},
  year    = {2022},
  url     = {https://github.com/demustamas/Mathemathical-Modeling-Practice},
  urldate = {2023-05-23},
}

@Online{_mav_,
  title   = {{MÁV} Központi Felépítményvizsgáló Kft.},
  url     = {http://www.mavkfv.hu/index.php?lngchg=en&f=},
  urldate = {2023-05-23},
}

'
@Misc{_ai_,
  title    = {{AI} {Research} {Group} – {Artificial} {Intelligence} \& {Data} {Science}},
  language = {en-US},
  url      = {https://ai.elte.hu/},
  urldate  = {2023-05-23},
}

'
@Misc{_ai&ml_,
  title    = {{AI}\&{ML} {Training} – {AI} {Research} {Group}},
  language = {en-US},
  url      = {https://ai.elte.hu/training/},
  urldate  = {2023-05-23},
}

@Online{_railway_,
  ranking    = {rank3},
  readstatus = {read},
  relevance  = {relevant},
  title      = {Railway Track Fault Detection {\textbar} Kaggle},
  url        = {https://www.kaggle.com/datasets/salmaneunus/railway-track-fault-detection},
  urldate    = {2022-12-22},
}

@Booklet{kfv_25years,
  title = {The MÁV Central Rail and Track Inspection Ltd. 25 years},
  year  = {2021},
  url   = {http://www.mavkfv.hu/index.php?f=kfv25},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

'
@Misc{_autoencoder_2023,
  month     = may,
  note      = {Page Version ID: 1153796242},
  title     = {Autoencoder},
  year      = {2023},
  abstract  = {An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.
Variants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language  = {en},
  url       = {https://en.wikipedia.org/w/index.php?title=Autoencoder&oldid=1153796242},
  urldate   = {2023-05-24},
}

'
@Misc{khosla_auto_2021,
  author     = {Khosla, Ritwek},
  month      = jan,
  title      = {Auto-{Encoders} for {Computer} {Vision}: {An} {Endless} world of {Possibilities}},
  year       = {2021},
  abstract   = {Auto-Encoders are sequential neural networks consisting of two components: an Encoder followed by a Decoder. Learn what are auto encoders.},
  journal    = {Analytics Vidhya},
  language   = {en},
  shorttitle = {Auto-{Encoders} for {Computer} {Vision}},
  url        = {https://www.analyticsvidhya.com/blog/2021/01/auto-encoders-for-computer-vision-an-endless-world-of-possibilities/},
  urldate    = {2023-05-24},
}

@Comment{jabref-meta: databaseType:bibtex;}
